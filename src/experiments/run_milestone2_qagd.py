# -*- coding: utf-8 -*-
import sys
import os
import torch
import matplotlib.pyplot as plt
import numpy as np

# Ajout du chemin pour trouver les modules
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.models.qagd.optimizer import QAGD

def run_experiment():
    print("\nüöÄ D√©marrage du Milestone 2 : Q-AGD Optimizer")
    print("Objectif : Minimiser une fonction non-convexe SANS calcul de gradient classique.\n")

    # --- 1. D√©finition du probl√®me ---
    # On utilise la fonction de Rosenbrock (c√©l√®bre test difficile pour les optimiseurs)
    # f(x, y) = (a - x)^2 + b * (y - x^2)^2
    # Minimum global en (a, a^2). Avec a=1, b=100 -> min en (1, 1) = 0.
    
    def rosenbrock(params):
        x, y = params[0], params[1]
        return (1 - x)**2 + 100 * (y - x**2)**2

    # Point de d√©part al√©atoire (loin de la solution 1,1)
    start_point = torch.tensor([-1.5, -1.0], requires_grad=False) # Pas de gradient n√©cessaire !
    params_qagd = start_point.clone()
    
    # --- 2. Configuration de l'optimiseur Q-AGD ---
    # Note : Pas besoin de "requires_grad=True" car Q-AGD estime le gradient lui-m√™me
    optimizer = QAGD([params_qagd], lr=0.5, perturbation=0.1)

    history = []
    
    print(f"Point de d√©part : {params_qagd.numpy()}")
    print("Lancement de l'optimisation...")

    # --- 3. Boucle d'optimisation ---
    for i in range(100):
        # La closure est la fonction qui permet √† l'optimiseur de r√©√©valuer la loss
        def closure():
            return rosenbrock(params_qagd)
        
        loss = optimizer.step(closure)
        history.append(loss.item())

        if i % 10 == 0:
            print(f"Iter {i:03d} | Loss: {loss.item():.6f} | Position: {params_qagd.numpy()}")

    # --- 4. R√©sultats ---
    print(f"\n‚úÖ Termin√©. Position finale : {params_qagd.numpy()}")
    print(f"Cible th√©orique : [1.  1.]")
    
    # Graphique
    try:
        plt.plot(history, label='Q-AGD Loss')
        plt.yscale('log') # √âchelle logarithmique pour mieux voir la convergence
        plt.title("Convergence de l'optimiseur Q-AGD (Rosenbrock)")
        plt.xlabel("It√©rations")
        plt.ylabel("Loss (Log Scale)")
        plt.legend()
        plt.grid(True, which="both", ls="--")
        plt.show()
    except:
        pass

if __name__ == "__main__":
    run_experiment()